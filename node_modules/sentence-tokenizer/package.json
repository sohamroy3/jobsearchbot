{
  "_from": "sentence-tokenizer@^1.0.1",
  "_id": "sentence-tokenizer@1.0.1",
  "_inBundle": false,
  "_integrity": "sha512-nmKF6fXmgZouD3AfWgYCmr35g7g7ObtbTlEFRVx2oj/ptrCOrosixrhXhWUdnPRdze7xhMf4IcliAa021BMXTA==",
  "_location": "/sentence-tokenizer",
  "_phantomChildren": {},
  "_requested": {
    "type": "range",
    "registry": true,
    "raw": "sentence-tokenizer@^1.0.1",
    "name": "sentence-tokenizer",
    "escapedName": "sentence-tokenizer",
    "rawSpec": "^1.0.1",
    "saveSpec": null,
    "fetchSpec": "^1.0.1"
  },
  "_requiredBy": [
    "/parramato"
  ],
  "_resolved": "https://registry.npmjs.org/sentence-tokenizer/-/sentence-tokenizer-1.0.1.tgz",
  "_shasum": "74e05e23bf42826c6f0bd4b60baaba38237adbe4",
  "_spec": "sentence-tokenizer@^1.0.1",
  "_where": "C:\\Users\\sroy1\\Documents\\JOBSEARCHBOT\\node_modules\\parramato",
  "author": {
    "name": "Fran√ßois Parmentier"
  },
  "bugs": {
    "url": "https://github.com/parmentf/node-sentence-tokenizer/issues"
  },
  "bundleDependencies": false,
  "dependencies": {
    "debug": "4.1.0"
  },
  "deprecated": false,
  "description": "Tokenize paragraphs into sentences, and smaller tokens.",
  "devDependencies": {
    "eslint": "5.9.0",
    "mocha": "5.2.0"
  },
  "homepage": "http://github.com/parmentf/node-sentence-tokenizer",
  "keywords": [
    "tokenizer",
    "sentence"
  ],
  "license": "MIT",
  "main": "lib/tokenizer.js",
  "name": "sentence-tokenizer",
  "repository": {
    "type": "git",
    "url": "git+https://github.com/parmentf/node-sentence-tokenizer.git"
  },
  "scripts": {
    "lint": "eslint lib",
    "test": "mocha",
    "test-w": "mocha -w"
  },
  "version": "1.0.1"
}
